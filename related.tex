\section{Related Work}
\label{sec:related}
Experiments with semi-supervised sarcasm identification on a Twitter dataset (5.9 million tweets) were conducted in \cite{davidov10}. They used 5-fold cross validation on their kNN-like classifier and obtained an F-measure of 0.55 on the Twitter dataset. In this paper \cite{davidov10}, authors use a semi-supervised sarcasm identification algorithm on a Twitter dataset and Amazon product reviews. In case of Twitter, authors mainly use $1500$ tweets containing \textit{\#sarcasm} hashtag and $180$ tweets tagged by $15$ Amazon Mechanical Turkers \cite{mturk} as golden test set or initial small labeled training set. The algorithm employs two modules: semi supervised pattern acquisition for identifying sarcastic patterns that serve as features for a classifier, and a classification stage that classifies each sentence to a sarcastic class. Reyes et al. \cite{reyes12} proposed features to capture properties of a figurative language such as ambiguity, polarity, unexpectedness and emotional scenarios. Their corpus consists of five categories (humor, irony, politics, technology and general), each containing 10,000 tweets. The best result in the classification of
irony and general tweets was F-measure 0.65. The work of Riloff et al. \cite{riloff13} identifies one type of sarcasm: contrast between a positive sentiment and negative situation. They used a bootstrapping algorithm to acquire lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. Their evaluation on a human-annotated dataset of 3000 tweets (23\% sarcastic) was done using the SVM classifier with uni-grams and bigrams as features, achieving an F-measure of 0.48. Tomas et. al. \cite{tomas14} also tried to employ different combinations of machine learning approaches using language independent specific feature set on Czech and English Twitter dataset (780,000 tweets) and achieved F-measure around 0.94.