\section{Related Work}
\textbf{Image based Facial Expression Detection : }In the past years, the literature on automatic facial expression recognition has grown dramatically by applying advanced techniques of image and video processing. Most studies of automatic facial expression recognition focus on six primary facial expressions or a subset of them, namely happiness, sadness, anger, fear, surprise, and disgust. The expression and recognition of these primary facial expressions were found in Ekman’s extensive studies\cite{ekman} to be universal in different cultures. The studies of computer-assisted recognition of facial expressions started in 1990s. Mase\cite{mase} explored the technique of optical flow for facial expressions recognition. Lanitis et al.\cite{lanitis} applied a flexible shape and appearance model to recognize person identities, genders and facial expressions. Black and Yacoob\cite{black95} used local parameterized models of image motion to track non-rigid facial motion that was fed to a rule-based classifier of facial expressions. Rosenblum et al.\cite{rosenblum} used optical flow and a radial basis function network to
classify expressions. Otsuka and Ohya\cite{otsuka97} used optical flow and a hidden Markov model (HMM) for facial expression
recognition. Tian et al\cite{tian} explored action unit recognition by using multi-state facial component models and a neural-network-based classifier. Cohen et al.\cite{cohen03} introduced the structure of Bayesian network classifiers and a multi-level HMM classifier to automatically segment an arbitrary long sequence to the corresponding facial expressions. For extensive survey of facial expression analysis using images done in the recent years, readers are referred to the overview papers, including \cite{pantic00,pantic03} written by Pantic and Rothkrantz in 2000 and 2003.\\

\textbf{EEG based Expression and Emotion Detection : }Several works have attempted recognition of emotions from EEG signals. In \cite{channel2009}, participants are asked to remember an episode in their life that corresponds to positive/excited and one that corresponds to negative/excited emotions. A third emotional state called calm/neutral is elicited by asking the participants to stay calm and relax. For these three classes, a classification accuracy of 63\% is reported using the short-time Fourier transform for feature extraction and a linear SVM for classification. In \cite{deap}, participants watch a series of music videos selected to elicit emotions. The participants then rate the felt emotions in terms of valence, arousal and like/dislike. In performing a binary classification, accuracies of up to 62\% are attained based on EEG band-power features and a Gaussian Naïve Bayes classifier. Regression results for the same experiment are reported in \cite{soleymani}. In \cite{takahashi}, 5 different emotions (joy, anger, sadness, fear, and relaxation) are elicited by using video stimuli in 12 participants. Using a one-vs-all SVM classifier, a classification rate of 41.7\% is reported. Besides these works, much research has been done in psychology into ERP analysis and correlations with emotion (e.g. \cite{olof,cuthbert}). These works show clear associations between ERP activity and valence/arousal. However, they mostly have in common that they work with time-locked stimuli (such as pictures), and average the ERP signal over several trials to increase the signal-to-noise ratio. However, all these works do not concentrate upon the facial expression detection cum emotion recognition which we attempt to address.