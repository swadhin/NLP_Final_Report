\section{Our Proposed Approach}
\label{sec:evaluation}
\noindent \textbf{Limitations of the Paper to be addressed :}
\begin{enumerate}
 \item Even the authors had 5.9 million tweets, they can only effectively use 1500 tweets as secondary training/test set and only 180 tweets as primary training/test set. Moreover, they achieve very poor precision of 0.545 in secondary dataset. So, in this project, we aim to crawl at least 10 thousand sarcastic tweets using Twitter's official search APIs via Tweepy \cite{tweepy} library. As there is rate limit to the apps per account in Twitter, we have already created multiple accounts to crawl in parallel to achieve the goal. If possible, we also hope to employ Amazon Mechanical Turk for data labelling and evaluation.
 \item In the paper, authors have not used any baseline for Twitter dataset as the semantic gap principle can not be easily adopted in case of Twitter. We aim to create a baseline for the Twitter dataset from this principle or some other recent work to compare our system's performance.
 \item Authors reduced the feature set for better pattern matching but they claim the better performance in Twitter dataset in second experiment is due to more number of features. So, the removal of hashtags, links etc. may reduce the context information (universal knowledge) in already restricted tweets of $140$ characters. We plan to gather the context information from the profile centric information of the users like follows, mentions, re-tweets, favorites, profile description, previous tweets etc. We believe that if we train any system with such context information then it will give better result in sarcasm detection as any kind of sarcasm assumes some sort of world knowledge.
 \item Interestingly, authors did not pursuit the idea of \textit{target} named entity in tweets as they did it in Amazon reviews. But, we believe that some classes of sarcastic tweets actually aimed at entities (e.g. may be person or institution or event or show) and if we can extract the sentiment regarding that we can get good indication of sarcasm. Moreover, some classes of sarcastic tweets may be general saying which is common across some classes of users (e.g. \textit{Yeah, I like Mondays more than my girlfriend}).
 \item Moreover, we can also use the lexical/morphological features like adding for example features based on latent semantics, topic models etc. to improve the accuracy, which the authors did not use. We may look at the effect of underlying social graph structure on the sarcastic nature of a particular tweet. Because, the social graph leaks the interest and expertise of the user and can hint at the tendencies of sarcasm to specific entities.\\
\end{enumerate}

\noindent \textbf{Main Idea :} In the discussed paper and all other relevant works, authors have attempted to classify tweets as an independent entity without incorporating the notion of world knowledge in some form. So, our intuition is that if we look into the user profile (In Twitter, some accounts are dedicated sarcastic fake accounts), his previous tweets, specific hashtags, specific mentions and the general sentiments of the named entities used in the tweet (e.g. \textit{Janet Jackson}, \textit{Twilight} the movie etc.), we can get more hints of the sarcastic nature of the tweets. Because, the restriction nature of tweets makes the outside contextual, temporal, and semantics meaning more important.\\

\noindent \textbf{Evaluation Plan :}
We have already collected the tweet sarcasm dataset used in \cite{davidov10} but it will not be readily used because the lack of any outside information. Moreover, we have also started to crawl \textit{Twitter} using search string \textit{\#sarcasm}, \textit{\#sarcastictweet}, and \textit{sarcasmimplied} via search API using tweepy\cite{tweepy} and collected around five thousand tweets. Thus, we plan to collect a reasonable amount of tweet with corresponding tweet centric information (like Geo IDs, Device IDs etc.), profile information, and one hop follower/followee information. Next, we plan to process all user, URL, hashtag, mention, emoticons of tweets according to the model of sarcasm detection mechanism. Tokenization of tweets requires proper handling of emoticons and other special character sequences
typical on Twitter which can be achieved using the ark-tweet-nlp tool \cite{gimpel11}. We also plan to extract named entity using Stanford Named Entity Recognition (NER) and also extract the sentiment about the entity collating all previous tweets about that entity. Next, we will try to use supervised machine learning algorithms like SVM, unsupervised algorithm like k-means clustering etc. with tuned and curated feature set. Finally, we will measure precision, recall, F-measure with different subset of features and investigate different trade-offs. 

\begin{table*}[htb]
%\renewcommand{\arraystretch}{1.9}
  \centering
  {\small
    %\vspace{-0.1in}
  \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
\hline
Feature set & Method used & Precision & Recall & Accuracy & F1-Score & AUC \\\hline
Bag-of-Words & SVM with linear kernel & $84.0475$ & $84.6621$ & $83.6177$ & $84.3674$ & $86.5413$ \\\hline
Word Bigram & SVM with linear kernel & $72.0175$ & $72.3411$ & $73.5127$ & $72.3132$ & $77.1137$  \\\hline
Word Trigram & SVM with linear kernel & $63.8823$ & $63.2914$ & $64.7732$ & $63.4913$  & $67.3412$  \\\hline
Only Lexical Feature & SVM with linear kernel & $62.0813$ & $62.2723$ & $65.1184$ & $62.3213$   & $68.9023$ \\\hline
Only Syntactic Feature & SVM with linear kernel & $58.5623$ & $59.4112$ & $61.5234$ & $58.8232$  & $66.6784$ \\\hline
All Features with Social Features & SVM with linear kernel & $\mathbf{95.2358}$ & $\mathbf{95.2218}$ & $\mathbf{96.6023}$ & $\mathbf{95.2123}$   & $\mathbf{97.7812}$ \\\hline
Bag-of-Words & Logitboost\footnote{http://www.cs.waikato.ac.nz/ml/weka/}with Decision Stump & $88.1239$ & $88.1239$ & $90.0324$ & $88.1239$  & $92.2013$  \\\hline
Word Bigram & Logitboost with Decision Stump & $73.3487$ & $74.2375$ & $76.5123$ & $74.2234$   & $79.7613$ \\\hline
Word Trigram & Logitboost with Decision Stump & $74.8812$ & $74.8812$ & $78.1276$ & $74.8812$   & $80.2314$ \\\hline
Only Lexical Feature & Logitboost with Decision Stump & $64.0823$ & $64.0823$ & $67.4098$ & $64.0823$  & $70.2349$  \\\hline
Only Syntactic Feature & Logitboost with Decision Stump & $60.6756$ & $60.6756$ & $63.5454$ & $60.6756$  & $64.0873$ \\\hline
All Features with Social Features & Logitboost with Decision Stump & $\mathbf{97.4568}$ & $\mathbf{97.4568}$ & $\mathbf{98.4176}$ & $\mathbf{97.4568}$ & $\mathbf{98.8876}$ \\\hline
Bag-of-Words & Bagging with SVM \footnote{http://homes.esat.kuleuven.be/~claesenm/ensemblesvm/}& $83.0475$ & $83.5611$ & $84.5177$ & $83.2674$ & $85.3413$ \\\hline
Word Bigram & Bagging with SVM & $71.2713$ & $71.4531$ & $73.3427$ & $71.3412$  & $75.1745$ \\\hline
Word Trigram & Bagging with SVM & $62.8845$ & $62.2914$ & $68.7732$ & $62.3457$ & $70.3456$ \\\hline
Only Lexical Feature & Bagging with SVM  & $58.0823$ & $58.0127$ & $61.2384$ & $58.0432$  & $63.4523$  \\\hline
Only Syntactic Feature & Bagging with SVM  & $53.1256$ & $53.1256$ & $56.8674$ & $53.1256$  & $59.0234$ \\\hline
All Features with Social Features & Bagging with SVM & $\mathbf{93.5812}$ & $\mathbf{93.1846}$ & $\mathbf{95.1456}$ & $\mathbf{93.4054}$  & $\mathbf{97.3216}$ \\\hline
%\vspace{-0.15in}
\end{tabular}
  }
 \vspace{0.05in}
  \caption{Summary of Results for running SVM, Boosting, and Bagging with different feature sets on datasets.}
%\vspace{-0.25in}
  \label{tab:data1}
\end{table*}


\begin{table}[htb]
%\renewcommand{\arraystretch}{1.9}
  \centering
  {\small
    %\vspace{-0.1in}
  \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
\hline
Technique & Precision & Recall & F1-Score & Accuracy\\\hline
SASI (CoNLL 2010) & $74.27$ & $70.67$ & $72.43$ & $78.68$\\\hline
Coling 2014 & $91.23$ & $91.23$ & $91.23$ & $95.77$\\\hline
SSD & $97.46$ & $97.46$ & $97.46$ & $98.89$\\\hline
%\vspace{-0.15in}
\end{tabular}
  }
 \vspace{0.05in}
  \caption{Comparing different supervised techniques of Sarcasm Detection in Twitter}
%\vspace{-0.25in}
  \label{tab:data2}
\end{table}

% \begin{table}[htb]
% %\renewcommand{\arraystretch}{1.9}
%   \centering
%   {\small
%     %\vspace{-0.1in}
%   \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
% \hline
% Technique & Precision & Recall & F1-Score & Accuracy\\\hline
% SASI (CoNLL 2010) & $50.67$ & $48.27$ & $49.44$ & $52.68$\\\hline
% Coling 2014 & $53.23$ & $53.23$ & $53.23$ & $59.77$\\\hline
% SSD & $65.46$ & $65.46$ & $65.46$ & $68.89$\\\hline
% %\vspace{-0.15in}
% \end{tabular}
%   }
%  \vspace{0.05in}
%   \caption{Comparing different techniques Sarcasm Detection in Twitter in Czech Dataset}
% %\vspace{-0.25in}
%   \label{tab:data2}
% \end{table}