\section{Evaluation and Discussion}
\label{sec:evaluation}
This is evaluation.

\begin{figure}[hbt]
\centering
\includegraphics[width=3.4in, height= 2.4 in]{./figs/bow.eps}
\caption{Effect of different language model specific training in different SSD classifiers}
\label{train:fig}
\end{figure}

\begin{figure}[hbt]
\centering
\includegraphics[width=3.4in, height= 2.4 in]{./figs/class.eps}
\caption{Comparison of Precision, Recall, F1-Score and Accuracy of different SSD classifiers}
\label{train:fig}
\end{figure}

\begin{table*}[htb]
%\renewcommand{\arraystretch}{1.9}
  \centering
  {\small
    %\vspace{-0.1in}
  \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
\hline
Feature set & Method used & Precision & Recall & Accuracy & F1-Score & AUC \\\hline
Bag-of-Words & SVM with linear kernel & $84.0475$ & $84.6621$ & $83.6177$ & $84.3674$ & $86.5413$ \\\hline
Word Bigram & SVM with linear kernel & $72.0175$ & $72.3411$ & $73.5127$ & $72.3132$ & $77.1137$  \\\hline
Word Trigram & SVM with linear kernel & $63.8823$ & $63.2914$ & $64.7732$ & $63.4913$  & $67.3412$  \\\hline
Only Lexical Feature & SVM with linear kernel & $62.0813$ & $62.2723$ & $65.1184$ & $62.3213$   & $68.9023$ \\\hline
Only Syntactic Feature & SVM with linear kernel & $58.5623$ & $59.4112$ & $61.5234$ & $58.8232$  & $66.6784$ \\\hline
All Features with Social Features & SVM with linear kernel & $\mathbf{95.2358}$ & $\mathbf{95.2218}$ & $\mathbf{96.6023}$ & $\mathbf{95.2123}$   & $\mathbf{97.7812}$ \\\hline
Bag-of-Words & Logitboost\footnote{http://www.cs.waikato.ac.nz/ml/weka/}with Decision Stump & $88.1239$ & $88.1239$ & $90.0324$ & $88.1239$  & $92.2013$  \\\hline
Word Bigram & Logitboost with Decision Stump & $73.3487$ & $74.2375$ & $76.5123$ & $74.2234$   & $79.7613$ \\\hline
Word Trigram & Logitboost with Decision Stump & $68.8812$ & $68.8812$ & $72.1276$ & $68.8812$   & $74.2314$ \\\hline
Only Lexical Feature & Logitboost with Decision Stump & $64.0823$ & $64.0823$ & $67.4098$ & $64.0823$  & $70.2349$  \\\hline
Only Syntactic Feature & Logitboost with Decision Stump & $60.6756$ & $60.6756$ & $63.5454$ & $60.6756$  & $64.0873$ \\\hline
All Features with Social Features & Logitboost with Decision Stump & $\mathbf{97.4568}$ & $\mathbf{97.4568}$ & $\mathbf{98.4176}$ & $\mathbf{97.4568}$ & $\mathbf{98.8876}$ \\\hline
Bag-of-Words & Bagging with SVM \footnote{http://homes.esat.kuleuven.be/~claesenm/ensemblesvm/}& $83.0475$ & $83.5611$ & $84.5177$ & $83.2674$ & $85.3413$ \\\hline
Word Bigram & Bagging with SVM & $71.2713$ & $71.4531$ & $73.3427$ & $71.3412$  & $75.1745$ \\\hline
Word Trigram & Bagging with SVM & $62.8845$ & $62.2914$ & $68.7732$ & $62.3457$ & $70.3456$ \\\hline
Only Lexical Feature & Bagging with SVM  & $58.0823$ & $58.0127$ & $61.2384$ & $58.0432$  & $63.4523$  \\\hline
Only Syntactic Feature & Bagging with SVM  & $53.1256$ & $53.1256$ & $56.8674$ & $53.1256$  & $59.0234$ \\\hline
All Features with Social Features & Bagging with SVM & $\mathbf{93.5812}$ & $\mathbf{93.1846}$ & $\mathbf{95.1456}$ & $\mathbf{93.4054}$  & $\mathbf{97.3216}$ \\\hline
%\vspace{-0.15in}
\end{tabular}
  }
 \vspace{0.05in}
  \caption{Summary of Results for running SVM, Boosting, and Bagging with different feature sets on datasets.}
%\vspace{-0.25in}
  \label{tab:data1}
\end{table*}

\begin{figure}[hbt]
\centering
\includegraphics[width=3.4in, height= 2.4 in]{./figs/compare_systems.eps}
\caption{Comparison of a few competing techniques with SSD}
\label{train:fig}
\end{figure}

\begin{table}[htb]
%\renewcommand{\arraystretch}{1.9}
  \centering
  {\small
    %\vspace{-0.1in}
  \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
\hline
Technique & Precision & Recall & F1-Score & Accuracy\\\hline
SASI \cite{davidov10} & $74.27$ & $70.67$ & $72.43$ & $78.68$\\\hline
Coling '14 \cite{tomas14} & $91.23$ & $91.23$ & $91.23$ & $95.77$\\\hline
SSD & $97.46$ & $97.46$ & $97.46$ & $98.89$\\\hline
%\vspace{-0.15in}
\end{tabular}
  }
 \vspace{0.05in}
  \caption{Comparing different supervised techniques of Sarcasm Detection in Twitter with SSD}
%\vspace{-0.25in}
  \label{tab:data2}
\end{table}

% \begin{table}[htb]
% %\renewcommand{\arraystretch}{1.9}
%   \centering
%   {\small
%     %\vspace{-0.1in}
%   \begin{tabular}{|@{~}l@{~~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|@{~~}l@{~}|}
% \hline
% Technique & Precision & Recall & F1-Score & Accuracy\\\hline
% SASI (CoNLL 2010) & $50.67$ & $48.27$ & $49.44$ & $52.68$\\\hline
% Coling 2014 & $53.23$ & $53.23$ & $53.23$ & $59.77$\\\hline
% SSD & $65.46$ & $65.46$ & $65.46$ & $68.89$\\\hline
% %\vspace{-0.15in}
% \end{tabular}
%   }
%  \vspace{0.05in}
%   \caption{Comparing different techniques Sarcasm Detection in Twitter in Czech Dataset}
% %\vspace{-0.25in}
%   \label{tab:data2}
% \end{table}