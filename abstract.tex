\begin{abstract}
This project proposes a system for automatically detecting different facial expressions of a user using EEG and accelerometer data recorded in a headband like \textit{Muse}. We observe that different channels of EEG combined with accelerometer data exhibits specific patterns which can help to identify a specific set of facial expressions. These expression set may help to tag user generated content, e.g. comments on a forum or messages on a messenger (like Whatsapp), with actual emotions of users. Our key observation is that different sensors available on smartphones and different wearables like brainwave sensing headbands could be used to capture a wide spectrum of user reactions or emotions. Especially actual physiological signatures (EEG) captured by Muse \cite{muse}, which gives more detailed information about the user, can be critical in this regard. We can map these EEG, accelerometer and other sensor data captured to infer the current emotional states of user's mind. To achieve this, we have built a DTW based pattern matching algorithm which benefits from the low-rank structure of sensing data. Through experiments, we have shown that our system achieves reasonable accuracy for detecting expressions based on six expression vocabulary. 
\end{abstract}
